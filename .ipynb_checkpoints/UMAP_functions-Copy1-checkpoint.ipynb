{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6bc550",
   "metadata": {},
   "source": [
    "# Function: umap_grid_search() \n",
    "This function will create multiple umaps with a variety of different parameters\n",
    "### Inputs\n",
    "- data = a numpy array of data (participants x data types)\n",
    "\n",
    "#### The following inputs are parameters for the UMAP - descriptions can be found here: https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "- metric = metric parameter\n",
    "- n_components = number of components frand\n",
    "- random_state = random state for reproducibility (https://umap-learn.readthedocs.io/en/latest/reproducibility.html)\n",
    "- n_neighbors_range = instead of a single value, this variable should be a range() of parameters to use\n",
    "- min_dist_range = this variable should be np.linspace(#, #, num=#) for evenly spaced minimum distance parameters\n",
    "\n",
    "### Output\n",
    "- results = dict containing all umaps with keys: n_neighbors, min_dist parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba3d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_grid_search(data,metric,n_components,random_state,n_neighbors_range,min_dist_range):\n",
    "    \"\"\"\n",
    "    \n",
    "    Create multiple UMAPs with input parameter combinations. Save UMAPs as a directionary (UMAP_results),\n",
    "    which keys identify which parameters were used to create that UMAP (nearest neighbors x minimum distance)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an empty dictionary to store the results\n",
    "    UMAP_results = {}\n",
    "    # Loop through different combinations of n_neighbors and min_dist\n",
    "    for n_neighbors in n_neighbors_range:\n",
    "        for min_dist in min_dist_range:\n",
    "            # Create a UMAP object\n",
    "            umap_model = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric,\n",
    "                              n_components=n_components, random_state=random_state)\n",
    "\n",
    "            # Fit and transform the scaled data using UMAP\n",
    "            umap_embedding = umap_model.fit_transform(data)\n",
    "\n",
    "            # Store the result with the corresponding parameters\n",
    "            UMAP_results[(n_neighbors, min_dist)] = umap_embedding\n",
    "    return UMAP_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1602bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_grid_search_procrustes(reference_data,reference_ages,data,data_ages,metric,n_components,random_state,n_neighbors_range,min_dist_range):\n",
    "    \"\"\"\n",
    "    \n",
    "    Create multiple UMAPs with input parameter combinations. This function includes a procrustes \n",
    "    transformation of the 'data' to the 'reference_data'. UMAPs are saved to the dictionary\n",
    "    'UMAP_results' and the key defines which parameters were used for that projection\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # First use 'ages' to align number of reference ages\n",
    "    reference_common_values = reference_ages[reference_ages.isin(data_ages)]\n",
    "    data_common_values = data_ages[data_ages.isin(reference_ages)]\n",
    "    # Filter reference_data and data based on common values\n",
    "    reference_data_filtered = reference_data[reference_common_values.index.values]\n",
    "    data_filtered = data[data_common_values.index.values]\n",
    "    \n",
    "    # Re-define n_neighbors_range to ensure the the maximum value doesn't exceed the reduced data length\n",
    "    if len(data_filtered) < len(data):\n",
    "        n_neighors_range = range(min(n_neighbors_range),len(data))  \n",
    "        print(f'Reduced n_neighbors_range = {n_neighbors_range}')\n",
    "    \n",
    "    # Create an empty dictionary to store the results\n",
    "    results = {}\n",
    "    # Loop through different combinations of n_neighbors and min_dist\n",
    "    for n_neighbors in n_neighbors_range:\n",
    "        for min_dist in min_dist_range:\n",
    "            # Create a UMAP object\n",
    "            umap_model = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric,\n",
    "                              n_components=n_components, random_state=random_state)\n",
    "\n",
    "            # Fit and transform the scaled data using UMAP\n",
    "            umap_embedding = umap_model.fit_transform(data_filtered)\n",
    "            reference_embedding = umap_model.fit_transform(reference_data_filtered)\n",
    "            \n",
    "            # Perform Procrustes transformation \n",
    "            transformation, _ = orthogonal_procrustes(umap_embedding, reference_embedding)\n",
    "            aligned_embedding = np.dot(umap_embedding, transformation)\n",
    "\n",
    "            # Store the result with the corresponding parameters\n",
    "            UMAP_results[(n_neighbors, min_dist)] = aligned_embedding\n",
    "    return UMAP_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb14850",
   "metadata": {},
   "source": [
    "# Function: missing_numbers()\n",
    "This function determines if there are any ages missing in provided ages\n",
    "### Input\n",
    "- consecutive_numbers = pandas series of the corresponding age data\n",
    "### Output\n",
    "- missing = the number of missing consecutive ages\n",
    "#### Example\n",
    "If consecutive_numbers = pd.Series([0,1,2,3,5,6,7]), then missing = 1 because age '4' is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_numbers(consecutive_numbers):\n",
    "    # Find the minimum and maximum values in the Series and cast to integers\n",
    "    min_val = int(consecutive_numbers.min())\n",
    "    max_val = int(consecutive_numbers.max())\n",
    "\n",
    "    # Check for missing numbers within the range\n",
    "    missing_numbers = [num for num in range(min_val, max_val + 1) if num not in consecutive_numbers.values]\n",
    "    missing = len(missing_numbers)\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be081a",
   "metadata": {},
   "source": [
    "# Function: calculate_turning_points()\n",
    "\n",
    "This function calculates turning points across all UMAPs in UMAP_results\n",
    "\n",
    "### Inputs\n",
    "- UMAP_results = dictionary containing all UMAPS\n",
    "- ages = average ages for the sample \n",
    "- degree = the degree for the line of best fit\n",
    "- age_window = window to average close inflection points \n",
    "- gradient_window = number of years around inflection points to sum gradients\n",
    "- gradient_threshold = the threshold for sum of gradients around the inflection points in order to be retained\n",
    "\n",
    "### Output\n",
    "- turning_point_results = a dict containing important results (e.g., major turning points and all turning points identified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f7e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_turning_points(UMAP_results, ages, degree, age_window, gradient_window, gradient_threshold):\n",
    "    \"\"\"\n",
    "    \n",
    "    This function takes a dictionary of UMAPS (results) and calculates turning points. This procedure\n",
    "    contains multiple steps:\n",
    "    Step 1: Create line lines of best fit through dimensions\n",
    "    Step 2: Find all inflection points for each line\n",
    "    Step 3: Remove small inflection points based on summed gradients within the gradient_window\n",
    "    Step 4: Find average turning points (averaging inflection points that are close together) \n",
    "    Step 5: Create KDE plot for visualization and to define major turning points\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dict for final turning points\n",
    "    all_turning_points = {}\n",
    "    all_manifolds = {}\n",
    "    for key, values in results.items():\n",
    "        # Organize data into seperate dimensions\n",
    "        x = values[:, 0]\n",
    "        y = values[:, 1]\n",
    "        z = values[:, 2]\n",
    "\n",
    "        #### STEP 1: Define line of best fit ####\n",
    "        # Determine number of missing ages\n",
    "        missing = missing_numbers(ages)\n",
    "        \n",
    "        # Create age range\n",
    "        curve_a = np.linspace(min(ages), max(ages), (len(ages)+missing))\n",
    "\n",
    "        # Fit X dimension\n",
    "        coefficientsX = np.polyfit(ages, x, degree)\n",
    "        curve_fitX = np.poly1d(coefficientsX)\n",
    "        curve_X = curve_fitX(curve_a)\n",
    "        # Fit Y dimension\n",
    "        coefficientsY = np.polyfit(ages, y, degree)\n",
    "        curve_fitY = np.poly1d(coefficientsY)\n",
    "        curve_Y = curve_fitY(curve_a)\n",
    "        # Fit Z dimension\n",
    "        coefficientsZ = np.polyfit(ages, z, degree)\n",
    "        curve_fitZ = np.poly1d(coefficientsZ)\n",
    "        curve_Z = curve_fitZ(curve_a)\n",
    "        # Save data\n",
    "        line_data = {'x': curve_X, 'y': curve_Y, 'z': curve_Z}\n",
    "        manifold = pd.DataFrame(line_data)\n",
    "        all_manifolds[key] = manifold\n",
    "        \n",
    "        #### STEP 2: Find all inflection points ####\n",
    "        # Calculate Gradient\n",
    "        manifold_gradients = np.gradient(manifold, axis=0)\n",
    "        manifold_gradients = pd.DataFrame(manifold_gradients, columns=['x', 'y', 'z'])\n",
    "\n",
    "        # Initialize \n",
    "        turning_points = {'x': [], 'y': [], 'z': []}\n",
    "        # Loop through gradients\n",
    "        for col in manifold_gradients:\n",
    "            signs = np.sign(manifold_gradients[col])    # Find sign of gradients (1 = positive, 0 = 0, -1 = negative)\n",
    "            sign_change = np.where(np.diff(signs))[0]   # Find ages where sign switches (elements not = 0 are where there is a difference in sign)\n",
    "            # Filter sign change indices to ensure they fall within the valid age index range\n",
    "            sign_change_reduced = [val for val in sign_change if val <= ages.index.max() and val >= ages.index.min()]\n",
    "            # Retrieve the corresponding ages for the valid sign change indices\n",
    "            ages_at_point = ages[sign_change_reduced]\n",
    "\n",
    "            # Remove instances of max/min of the given age range\n",
    "            ages_at_point = [age for age in ages_at_point if age not in [min(ages), max(ages)]]\n",
    "            \n",
    "            # Save ages where sign changes to specific dimension\n",
    "            if col == 'x':\n",
    "                turning_points['x'] = ages_at_point\n",
    "            elif col == 'y':\n",
    "                turning_points['y'] = ages_at_point\n",
    "            elif col == 'z':\n",
    "                turning_points['z'] = ages_at_point\n",
    "\n",
    "        #### STEP 3: Remove all small inflection points ####\n",
    "        # Initialize\n",
    "        turning_gradients = {'x': [], 'y': [], 'z': []}\n",
    "        # Loop through turning points\n",
    "        for k in turning_points.keys():\n",
    "            grad_col = manifold_gradients[k]\n",
    "            \n",
    "            # Loop through turning points for a specific dimension\n",
    "            for age_point in turning_points[k]:\n",
    "                # Find age window around turning point\n",
    "                search_range = range((int(age_point) - gradient_window), (int(age_point) + gradient_window))\n",
    "                # Initialize variable to store gradients\n",
    "                grad = 0\n",
    "                # Loop through all ages in age widnow\n",
    "                for a in search_range:\n",
    "                    if grad_col.index.min() <= a <= grad_col.index.max(): # If age is within the age range \n",
    "                        grad += abs(grad_col.iloc[a]) # Take the absolute value of gradient and add to 'grad' list\n",
    "                    else:\n",
    "                        continue # Ignore values outside the age range\n",
    "                # Save sum of gradients to specific dimensions\n",
    "                if k == 'x':\n",
    "                    turning_gradients['x'].append(grad)\n",
    "                elif k == 'y':\n",
    "                    turning_gradients['y'].append(grad)\n",
    "                elif k == 'z':\n",
    "                    turning_gradients['z'].append(grad)\n",
    "\n",
    "        # Re-organize list into one dataframe for all dimensions\n",
    "        # Initialize\n",
    "        turns = []\n",
    "        gradients = []\n",
    "        # Loop through points\n",
    "        for points_list in turning_points.values():\n",
    "            turns.extend(points_list)\n",
    "        # Loop through gradients\n",
    "        for points_list in turning_gradients.values():\n",
    "            gradients.extend(points_list)\n",
    "            \n",
    "        # Save\n",
    "        points = pd.DataFrame({'Turning Points': turns, 'Gradients': gradients})\n",
    "\n",
    "        # Filter out any turning points where the sum of gradients is equal to or below the given threshold\n",
    "        points_reduced = points[points['Gradients'] > int(gradient_threshold)]\n",
    "        \n",
    "        # Order turning points by age\n",
    "        points_reduced = points_reduced.sort_values(by='Turning Points').reset_index(drop=True)\n",
    "\n",
    "        #### STEP 4: Average turning points that are close together ####\n",
    "        # Initialize                    \n",
    "        group = []\n",
    "        final_turning_points = []\n",
    "        # Loop through all turning points     \n",
    "        for i in range(len(points_reduced)): \n",
    "            # Select age at turning point\n",
    "            var = points_reduced.iloc[i, 0]\n",
    "\n",
    "            if i + 1 >= len(points_reduced): # If this iteration the last turning point\n",
    "                final_turning_points.append(var) # Add point to the list and stop loop\n",
    "                break\n",
    "            else:\n",
    "                var2 = points_reduced.iloc[i + 1, 0] # Take next turning point in the list\n",
    "\n",
    "            # If 'group' already has a turning point saved\n",
    "            if len(group) >= 1: \n",
    "                # If the second turning point is within the given age window\n",
    "                if var2 - var <= age_window: \n",
    "                    group.append(var2)       # Save second turning point to list\n",
    "                    \n",
    "                # If the difference between turning points is greater than the given window\n",
    "                else:    \n",
    "                    final_turning_points.append(np.mean(group)) # Take the average of all turning points in the list and save it as final\n",
    "                    group = [] # Reset group variable\n",
    "            # If 'group' is empty\n",
    "            else: \n",
    "                group = [var] # Start a new group by adding current variable to the list\n",
    "                \n",
    "                # If the second turning point is within the given age window\n",
    "                if var2 - var <= 5:\n",
    "                    group.append(var2)\n",
    "                # If the difference between turning points is greater than the given window\n",
    "                else:\n",
    "                    final_turning_points.append(np.mean(group)) # Take the average of all turning points in the list and save it as final\n",
    "                    group = [] # Reset group variable\n",
    "        \n",
    "        # Round turning point ages and save for each parameter key\n",
    "        points = np.round(final_turning_points)\n",
    "        all_turning_points[key] = points\n",
    "        \n",
    "        ### If you want to identify specific manifolds with turning points similar to the ones identified \n",
    "        ### you can uncomment this loop and it will tell you which keys (UMAP parameters) yield manifolds\n",
    "        ### with similar turning points you define.\n",
    "        \n",
    "        #if len(points) == 4: # Define the number of target turning points\n",
    "            ## Define the target numbers\n",
    "            #target_numbers = [8, 32, 62, 85] # Change these to the the turning points found\n",
    "\n",
    "            ## Check if final_turning_points includes numbers within 1 of the target numbers\n",
    "            #if all(abs(num - target) <= 5 for num, target in zip(points, target_numbers)):\n",
    "                #print(key)\n",
    "                    \n",
    "    #### STEP 5: Visualize and define major turning points ####\n",
    "    \n",
    "    # Extract turning points for all UMAPS and add to one list\n",
    "    age_values = [] # Initalize list\n",
    "    # Loop through all keys\n",
    "    for key in all_turning_points:\n",
    "        vals = all_turning_points[key] # Pull turning points for one projection\n",
    "        age_values.extend(vals)        # Add turning points to the list\n",
    "\n",
    "    # Use KDE to define top turning points\n",
    "    kde = stats.gaussian_kde(age_values)\n",
    "    # Create continues age variable\n",
    "    x_values = np.linspace(min(age_values), max(age_values), len(age_values))\n",
    "    # Calculate the KDE values for the given x_values\n",
    "    kde_values = kde(x_values)\n",
    "    # Compute the gradient of the KDE values to identify turning points\n",
    "    kde_derivative = np.gradient(kde_values)\n",
    "    # Find the indices where the gradients changes sign from positive to negative (high frequency ages)\n",
    "    kde_turning_points = np.where(np.diff(np.sign(kde_derivative)) < 0)[0]\n",
    "    # Find associated age at turning point \n",
    "    final_turning_points = np.round(x_values[kde_turning_points])\n",
    "    \n",
    "    # Count how many occurances of turning points are present \n",
    "    turning_point_count = []\n",
    "    for i in range(len(final_turning_points)):\n",
    "        turning_point_count.append(np.count_nonzero(age_values == final_turning_points[i]))\n",
    "    num_turning_points = len(age_values)\n",
    "\n",
    "    # Save results\n",
    "    turning_point_results = {\n",
    "        \"major_turning_points\": final_turning_points, # List of major turning points calculated from KDE\n",
    "        \"all_manifolds\": all_manifolds,               # Fitted lines through all manifolds\n",
    "        \"all_turning_points\": all_turning_points,     # All turning points seperated into the manifold they were identified in\n",
    "        \"age_values\": age_values,                     # List of all turning points identified across all manifolds\n",
    "        \"num_turning_points\": num_turning_points,     # Number of turning points identified across all manifolds\n",
    "        \"turning_point_count\": turning_point_count,   # Number of times each major turning point was identified\n",
    "        \"x_values\": x_values,                         # Continous range of ages for KDE plot\n",
    "        \"kde_values\": kde_values,                     # KDE estimate for each value in x_values\n",
    "        \"kde_turning_points\": kde_turning_points      # Indices of KDE plot inflection points\n",
    "    }\n",
    "    \n",
    "    return turning_point_results\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
